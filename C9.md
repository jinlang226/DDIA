一致性与共识  Consistency and Consensus

分布式系统最重要的抽象之一就是共识（consensus）：就是让所有的节点对某件事达成一致。


# 一致性保证 Consistency Guarantees
* A better name for **eventual consistency** may be **convergence**, as we expect all replicas to eventually converge to the same value
* **事务隔离**主要是为了，避免由于同时执行事务而导致的竞争状态，而**分布式一致性**主要关于，面对延迟和故障时，如何协调副本间的状态。

# 线性一致性 Linearizability
* The basic idea is to make a system appear as if there were only one copy of the data, and all operations on it are atomic.
* Maintaining the illusion of a single copy of the data means guaranteeing that the value read is the most recent, up-to-date value, and doesn’t come from a stale cache or replica.
* 最新的，而不是来自陈旧的缓存或副本。换句话说，线性一致性是一个**新鲜度保证（recency guarantee）**。

## What Makes a System Linearizable?
* $read(x)⇒v$表示客户端请求读取寄存器 x 的值，数据库返回值 v。
* $write(x,v)⇒r$ 表示客户端请求将寄存器 x 设置为值 v ，数据库返回响应 r （可能正确，可能错误）。
* $cas(x, v_{old}, v_{new})⇒r$ 表示客户端请求进行原子性的比较与设置操作。如果寄存器 $x$ 的当前值等于 $v_{old}$ ，则应该原子地设置为 $v_{new}$ 。如果 $x \neq v_{old}$ ，则操作应该保持寄存器不变并返回一个错误。 $r$ 是数据库的响应（正确或错误）。
* 通过记录所有请求和响应的时序，并检查它们是否可以排列成有效的顺序，测试一个系统的行为是否线性一致性是可能的（尽管在计算上是昂贵的）

## 线性一致性与可序列化
* 可序列化（Serializability）是事务的隔离属性，每个事务可以读写多个对象（行，文档，记录）——参阅“单对象和多对象操作”。它确保事务的行为，与它们按照某种顺序依次执行的结果相同（每个事务在下一个事务开始之前运行完成）。这种执行顺序可以与事务实际执行的顺序不同。
* 线性一致性（Linearizability）是读取和写入寄存器（单个对象）的新鲜度保证。它不会将操作组合为事务，因此它也不会阻止写偏差等问题

## Relying on Linearizability

### Locking and leader election 锁定和领导选举
* Coordination services like Apache ZooKeeper [15] and etcd [16] are often used to implement distributed locks and leader election.
* Distributed locking is also used at a much more granular level in some distributed databases, such as Oracle Real Application Clusters (RAC) 

### Constraints and uniqueness guarantees 约束和唯一性保证
例如，用户名或电子邮件地址必须唯一标识一个用户

### Cross-channel timing dependencies 跨信道的时序依赖
首先将照片写入文件存储服务，写入完成后再将缩放器的指令放入消息队列。

## Implementing Linearizable Systems 实现线性一致的系统
使系统容错最常用的方法是使用复制。我们再来回顾第5章中的复制方法，并比较它们是否可以满足线性一致性：

#### 单主复制（可能线性一致）Single-leader replication (potentially linearizable)
* 如果从主库或同步更新的从库读取数据，它们可能（protential）是线性一致性的iv。然而，并不是每个单主数据库都是实际线性一致性的，无论是通过设计（例如，因为使用快照隔离）还是并发错误
* 一个节点很可能会认为它是领导者，而事实上并非如此——如果具有错觉的领导者继续为请求提供服务，可能违反线性一致性

#### 共识算法（线性一致）Consensus algorithms (linearizable)
共识协议包含防止脑裂和陈旧副本的措施。zookeeper，etcd

#### 多主复制（非线性一致）Multi-leader replication (not linearizable)
因为它们同时在多个节点上处理写入，并将其异步复制到其他节点。因此，它们可能会产生冲突的写入，需要解析

#### 无主复制（也许不是线性一致的）Leaderless replication (probably not linearizable)
* 有时候人们会声称通过要求法定人数读写（ $w + r> n$ ）可以获得“强一致性”。这取决于法定人数的具体配置，以及强一致性如何定义。eg：Dynamo
* 基于时钟（例如，在Cassandra中；参见“依赖同步时钟”）的“最后写入胜利”冲突解决方法几乎可以确定是非线性的，由于时钟偏差，不能保证时钟的时间戳与实际事件顺序一致。eg：Cassandra

### 线性一致性和法定人数 Linearizability and quorums
严格的法定人数读写应该是线性一致性的。但是当我们有可变的网络延迟时，就可能存在竞争条件

## 线性一致性的代价 The Cost of Linearizability
* 一些复制方法可以提供线性一致性，另一些复制方法则不能
* 使用多主数据库，每个数据中心都可以继续正常运行
* 如果使用单主复制，则主库必须位于其中一个数据中心。如果客户端可以直接连接到主库所在的数据中心，这就不是问题了，哪些应用可以继续正常工作。但直到网络链接修复之前，只能访问从库数据中心的客户端会中断运行。

### The CAP theorem, Consistency, Availability, Partition tolerance
* 问题面临的权衡如下：
    * 如果应用需要线性一致性，且某些副本因为网络问题与其他副本断开连接，那么这些副本掉线时不能处理请求。请求必须等到网络问题解决，或直接返回错误。（无论哪种方式，服务都不可用（unavailable））。
    * 如果应用不需要线性一致性，那么某个副本即使与其他副本断开连接，也可以独立处理请求（例如多主复制）。在这种情况下，应用可以在网络问题前保持可用，但其行为不是线性一致的。
* 不需要线性一致性的应用对网络问题有更强的容错能力。这种见解通常被称为CAP定理
* CAP定理鼓励数据库工程师向分布式无共享系统的设计领域深入探索，这类架构更适合实现大规模的网络服务。 对于这种文化上的转变，CAP值得赞扬 —— 它见证了自00年代中期以来新数据库的技术爆炸（即NoSQL）。
* CAP定理的正式定义仅限于很狭隘的范围，它只考虑了一个一致性模型（即线性一致性）和一种故障（网络分区，或活跃但彼此断开的节点）。没有讨论任何关于网络延迟，死亡节点或其他权衡的事。 因此，尽管CAP在历史上有一些影响力，但对于设计系统而言并没有实际价值
* When a network fault occurs, you have to choose between either linearizability or total availability. Thus, a better way of phras‐ ing CAP would be either Consistent or Available when Partitioned

### 线性一致性和网络延迟 Linearizability and network delays
* 现代多核CPU上的内存甚至都不是线性一致的
* 除非使用：内存屏障（memory barrier）或围栏（fence）
* 每个CPU核都有自己的内存缓存和存储缓冲区
* 许多分布式数据库也是如此：它们是**为了提高性能**而选择了牺牲线性一致性，而不是为了容错
* 如果你想要线性一致性，读写请求的响应时间至少与网络延迟的不确定性成正比

## 顺序保证 Ordering Guarantees
顺序，线性一致性和共识之间有着深刻的联系

### 顺序与因果 Ordering and Causality
* 顺序反复出现有几个原因，其中一个原因是，它有助于保持因果关系（causality）
* 因果关系对事件施加了一种顺序：因在果之前；消息发送在消息收取之前。
* 如果一个系统服从因果关系所规定的顺序，我们说它是因果一致（causally）的。例如，快照隔离提供了因果一致性：当你从数据库中读取到一些数据时，你一定还能够看到其因果前驱（假设在此期间这些数据还没有被删除）。

### 因果顺序不是全序的 The causal order is not a total order
* ​全序（total order）允许任意两个元素进行比较。​ 然而数学集合并不完全是全序的：{a, b} 比 {b, c} 更大吗？们说它们是无法比较（incomparable）的，因此数学集合是偏序（partially order）的
* 全序和偏序之间的差异反映在不同的数据库一致性模型中：
    * 线性一致性：全序
    * 因果性：如果两个时间因果相关，则有序。但如果并发，是偏序
* 在线性一致的数据存储中是不存在并发操作的：必须有且仅有一条时间线，所有的操作都在这条时间线上，构成一个全序关系
* 并发意味着时间线会分岔然后合并，不同分支上的操作是无法比较的（即并发操作）
* 例子：Git

### 线性一致性强于因果一致性
* 线性一致性隐含着（implies）因果关系：任何线性一致的系统都能正确保持因果性
* 使系统线性一致可能会损害其性能和可用性
* 线性一致性并不是保持因果性的唯一途径 —— 还有其他方法。看上去需要线性一致性的系统，实际上需要的只是因果一致性

### 捕获因果关系 Capturing causal dependencies
* In order to maintain causality, you need to know which operation happened before which other operation. This is a partial order: concurrent operations may be processed in any order, but if one operation happened before another, then they must be processed in that order on every replica.
* 它需要跟踪整个数据库中的因果依赖，而不仅仅是一个键
* 为了确定因果顺序，数据库需要知道应用读取了哪个版本的数据。这就是为什么在 图5-13 中，来自先前操作的版本号在写入时被传回到数据库的原因。在SSI 的冲突检测中会出现类似的想法，如“可序列化的快照隔离（SSI）”中所述：当事务要提交时，数据库将检查它所读取的数据版本是否仍然是最新的。为此，数据库跟踪哪些数据被哪些事务所读取。

## 序列号顺序 Sequence Number Ordering
我们可以使用**序列号（sequence nunber）**或**时间戳（timestamp）**来排序事件。时间戳不一定来自时钟（或物理时钟，存在许多问题，如 “不可靠时钟” 中所述）。它可以来自一个**逻辑时钟（logical clock）**，这是一个用来生成标识操作的数字序列的算法，典型实现是使用一个每次操作自增的计数器。

### 非因果序列号生成器 Noncausal sequence number generators
* 如果主库不存在（可能因为使用了多主数据库或无主数据库，或者因为使用了分区的数据库），如何为操作生成序列号就没有那么明显了。在实践中有各种各样的方法：奇偶，屋里适中，预先分配序号列区块
* 这三个选项都比单一主库的自增计数器表现要好，并且更具可扩展性。它们为每个操作生成一个唯一的，近似自增的序列号。然而它们都有同一个问题：生成的序列号与因果不一致。

### 兰伯特时间戳 Lamport timestamps
* 有一个简单的方法来产生与因果关系一致的序列号。它被称为兰伯特时间戳
* 每个节点都有一个唯一标识符，和一个保存自己执行操作数量的计数器。 兰伯特时间戳就是两者的简单组合：（计数器，节点ID）$(counter, node ID)$。两个节点有时可能具有相同的计数器值，但通过在时间戳中包含节点ID，每个时间戳都是唯一的。
* ​兰伯特时间戳与物理时间时钟没有任何关系，但是它提供了一个全序：如果你有两个时间戳，则计数器值大者是更大的时间戳。如果计数器值相同，则节点ID越大的，时间戳越大。

### 光有时间戳排序还不够 Timestamp ordering is not sufficient
* 为了确保没有其他节点正在使用相同的用户名和较小的时间戳* 并发创建同名账户，你必须检查其它每个节点，看看它在做什么【56】。如果其中一个节点由于网络问题出现故障或不可达，则整个系统可能被拖至停机。这不是我们需要的那种容错系统。
* 还需要知道这个全序何时会尘埃落定。如果你有一个创建用户名的操作，并且确定在全序中，没有任何其他节点可以在你的操作之前插入对同一用户名的声称，那么你就可以安全地宣告操作执行成功。

## 全序广播 Total Order Broadcast
* However, in a distributed system, getting all nodes to agree on the same total ordering of operations is tricky.
* 单主复制通过选择一个节点作为主库来确定操作的全序，并在主库的单个CPU核上对所有操作进行排序。
* 如果吞吐量超出单个主库的处理能力，这种情况下如何扩展系统；以及，如果主库失效（“处理节点宕机”），如何处理故障切换。在分布式系统文献中，这个问题被称为**全序广播（total order broadcast）**或**原子广播（atomic broadcast）**
* 全序广播通常被描述为在节点间交换消息的协议。 非正式地讲，它要满足两个安全属性：
    * 可靠交付（reliable delivery）
        * 没有消息丢失：如果消息被传递到一个节点，它将被传递到所有节点。
    * 全序交付（totally ordered delivery）
        * 消息以相同的顺序传递给每个节点。
* 正确的全序广播算法必须始终保证可靠性和有序性，即使节点或网络出现故障。当然在网络中断的时候，消息是传不出去的，但是算法可以不断重试，以便在网络最终修复时，消息能及时通过并送达（当然它们必须仍然按照正确的顺序传递）。

### 使用全序广播
* 像ZooKeeper和etcd这样的共识服务实际上实现了全序广播
* 全序广播正是数据库复制所需的：如果每个消息都代表一次数据库的写入，且每个副本都按相同的顺序处理相同的写入，那么副本间将相互保持一致（除了临时的复制延迟）。这个原理被称为状态机复制（state machine replication）
* 顺序在消息送达时被固化：如果后续的消息已经送达，节点就不允许追溯地将（先前）消息插入顺序中的较早位置。这个事实使得全序广播比时间戳命令更强
* 全序广播对于实现提供防护令牌的锁服务也很有用，列号可以当成防护令牌用，因为它是单调递增的。在ZooKeeper中，这个序列号被称为zxid

### 使用全序广播实现线性一致的存储 Implementing linearizable storage using total order broadcast
* 全序广播是异步的：消息被保证以固定的顺序可靠地传送，但是不能保证消息何时被送达（所以一个接收者可能落后于其他接收者）。相比之下，线性一致性是新鲜性（recent）的保证：读取一定能看见最新的写入值。
* 但如果有了全序广播，你就可以在此基础上构建线性一致的存储。例如，你可以确保用户名能唯一标识用户帐户。
* 你可以通过将全序广播当成仅追加日志的方式来实现这种线性一致的CAS操作
* 尽管这一过程保证写入是线性一致的，但它并不保证读取也是线性一致的 —— 如果你从与日志异步更新的存储中读取数据，结果可能是陈旧的。 
* 精确地说，这里描述的过程提供了顺序一致性（sequential consistency），有时也称为时间线一致性（timeline consistency），比线性一致性稍微弱一些的保证。

### 使用线性一致性存储实现全序广播 Implementing linearizable storage using total order broadcast
* 假设我们有线性一致的存储，如何在此基础上构建全序广播。
* 最简单的方法是假设你有一个线性一致的寄存器来存储一个整数，并且有一个原子自增并返回操作
* 与兰伯特时间戳不同，通过自增线性一致性寄存器获得的数字形式上是一个没有间隙的序列。因此，如果一个节点已经发送了消息 4 并且接收到序列号为 6 的传入消息，则它知道它在传递消息 6 之前必须等待消息 5 。兰伯特时间戳则与之不同 —— 事实上，这是全序广播和时间戳排序间的关键区别。
* 线性一致的CAS（或自增并返回）寄存器与全序广播都都等价于**共识**问题

# 分布式事务与共识 Distributed Transactions and Consensus
目标只是让几个节点达成一致（get serveral nodes to agree on something）。节点能达成一致，在很多场景下都非常重要，例如：
* 领导选举：在单主复制的数据库中，所有节点需要就哪个节点是领导者达成一致。
* 原子提交：​ 在支持跨多节点或跨多分区事务的数据库中，一个事务可能在某些节点上失败，但在其他节点上成功。如果我们想要维护事务的原子性，我们必须让所有节点对事务的结果达成一致：要么全部中止/回滚（如果出现任何错误），要么它们全部提交（如果没有出错）。这个共识的例子被称为原子提交（atomic commit）问题。

## Atomic Commit and Two-Phase Commit (2PC)
原子性可以防止失败的事务搅乱数据库，避免数据库陷入半成品结果和半更新状态。原子性确保二级索引与主数据保持一致（如果索引与主数据不一致，就没什么用了）。

### 从单节点到分布式原子提交 From single-node to distributed atomic commit
* 事务的提交主要取决于数据持久化落盘的顺序：首先是数据，然后是提交记录
* 在单个节点上，事务的提交主要取决于数据持久化落盘的顺序：首先是数据，然后是提交记录
* 但是，如果一个事务中涉及多个节点呢？仅向所有节点发送提交请求并独立提交每个节点的事务是不够的。这样很容易发生违反原子性的情况：提交在某些节点上成功，而在其他节点上失败
* 而且一旦在某个节点上提交了一个事务，如果事后发现它在其它节点上被中止了，它是无法撤回的。出于这个原因，一旦确定事务中的所有其他节点也将提交，节点就必须进行提交。

### 两阶段提交简介 Introduction to two-phase commit
易混概念：2PC在分布式数据库中提供原子提交，而2PL提供可序列化的隔离等级。为了避免混淆，最好把它们看作完全独立的概念，并忽略名称中不幸的相似性。
* 用于实现跨多个节点的原子事务提交的算法，即确保所有节点提交或所有节点中止。 
* PC在某些数据库内部使用，也以XA事务的形式对应用可用（例如Java Transaction API支持）或以SOAP Web服务的WS-AtomicTransaction 形式提供给应用
* ​2PC使用一个通常不会出现在单节点事务中的新组件：协调者（coordinator）（也称为事务管理器（transaction manager））。
* 正常情况下，2PC事务以应用在多个数据库节点上读写数据开始。我们称这些数据库节点为参与者（participants）。当应用准备提交时，协调者开始阶段 1 ：它发送一个准备（prepare）请求到每个节点，询问它们是否能够提交。然后协调者会跟踪参与者的响应：
    * 如果所有参与者都回答“是”，表示它们已经准备好提交，那么协调者在阶段 2 发出提交（commit）请求，然后提交真正发生。
    * 如果任意一个参与者回复了“否”，则协调者在阶段2 中向所有节点发送中止（abort）请求。

### 系统承诺 A system of promises
该协议包含两个关键的“不归路”点：当参与者投票“是”时，它承诺它稍后肯定能够提交（尽管协调者可能仍然选择放弃）。一旦协调者做出决定，这一决定是不可撤销的。这些承诺保证了2PC的原子性。 （单节点原子提交将这两个事件混为一谈：将提交记录写入事务日志。）

### 协调者失效 Coordinator failure
* 如果协调者在发送准备请求之前失败，参与者可以安全地中止事务。但是，一旦参与者收到了准备请求并投了“是”，就不能再单方面放弃 —— 必须等待协调者回答事务是否已经提交或中止。如果此时协调者崩溃或网络出现故障，参与者什么也做不了只能等待。参与者的这种事务状态称为存疑（in doubt）的或不确定（uncertain）的。
* 协调者必须在向参与者发送提交或中止请求之前，将其提交或中止决定写入磁盘上的事务日志：协调者恢复后，通过读取其事务日志来确定所有存疑事务的状态。任何在协调者日志中没有提交记录的事务都会中止。
* the commit point of 2PC comes down to a regular single-node atomic commit on the coordinator.

### 三阶段提交 Three-phase commit
* 两阶段提交被称为**阻塞（blocking）** 原子提交协议（protocol），因为存在2PC可能卡住并等待协调者恢复的情况。理论上，可以使一个原子提交协议变为 **非阻塞（nonblocking）** 的，以便在节点失败时不会卡住。
* 3PC假定网络延迟有界，节点响应时间有限；在大多数具有无限网络延迟和进程暂停的实际系统中，它并不能保证原子性。
* 非阻塞原子提交需要一个完美的**故障检测器（perfect failure detector）**，即一个可靠的机制来判断一个节点是否已经崩溃。在具有无限延迟的网络中，超时并不是一种可靠的故障检测机制，因为即使没有节点崩溃，请求也可能由于网络问题而超时。出于这个原因，2PC仍然被使用，尽管大家都清楚可能存在协调者故障的问题。

## 实践中的分布式事务
**分布式事务**的含义。两种截然不同的分布式事务类型经常被混淆：
* 数据库内部的分布式事务 Database-internal distributed transactions
  * 支持数据库节点之间的内部事务。例如，VoltDB和MySQL Cluster的NDB存储引擎
* 异构分布式事务 Heterogeneous distributed transactions
  * 参与者是两种或以上不同技术
  * 数据库内部事务不必与任何其他系统兼容，因此它们可以使用任何协议，并能针对特定技术进行特定的优化。因此数据库内部的分布式事务通常工作地很好。另一方面，跨异构技术的事务则更有挑战性。

### 恰好一次的消息处理 Exactly-once message processing
* 异构的分布式事务处理能够以强大的方式集成不同的系统。
* 通过在同一个事务中原子提交消息确认和数据库写入两个操作来实现
* If either the message delivery or the database transaction fails, both are aborted, and so the message broker may safely redeliver the message later. Thus, by atomically committing the message and the side effects of its processing, we can ensure that the message is effectively processed exactly once, even if it required a few retries before it succeeded. The abort discards any side effects of the partially completed transaction.

### XA事务
* X/Open XA (short for eXtended Architecture) is a standard for implementing two-phase commit across heterogeneous technologies 
* 许多传统关系数据库（包括PostgreSQL，MySQL，DB2，SQL Server和Oracle）和消息代理message broker（包括ActiveMQ，HornetQ，MSMQ和IBM MQ） 都支持XA
* XA不是一个网络协议——它只是一个用来与事务协调者连接的C API。
* 事务协调者需要实现XA API。标准没有指明应该如何实现，但实际上协调者通常只是一个库，被加载到发起事务的应用的同一个进程中（而不是单独的服务）。
* 如果应用进程崩溃，或者运行应用的机器报销了，协调者也随之往生极乐。然后任何带有准备了但未提交事务的参与者都会在疑虑中卡死。由于协调程序的日志位于应用服务器的本地磁盘上，因此必须重启该服务器，且协调程序库必须读取日志以恢复每个事务的提交/中止结果。
* 数据库服务器不能直接联系协调者，因为所有通信都必须通过客户端库。

### 怀疑时持有锁 Holding locks while in doubt
* 在事务提交或中止之前，数据库不能释放这些锁
* 当这些锁被持有时，其他事务不能修改这些行。根据数据库的不同，其他事务甚至可能因为读取这些行而被阻塞。因此，其他事务没法儿简单地继续它们的业务了 —— 如果它们要访问同样的数据，就会被阻塞。这可能会导致应用大面积进入不可用状态，直到存疑事务被解决。

### 从协调者故障中恢复 Recovering from coordinator failure
* 理论上，如果协调者崩溃并重新启动，它应该干净地从日志中恢复其状态，并解决任何存疑事务。然而在实践中，**孤立（orphaned）** 的存疑事务确实会出现，即无论出于何种理由，协调者无法确定事务的结果（例如事务日志已经由于软件错误丢失或损坏）。这些事务无法自动解决，所以它们永远待在数据库中，持有锁并阻塞其他事务。
* 重启数据库服务器也无法解决这个问题，因为在2PC的实现中，重启必须保留存疑事务的锁。
* 管理员手动决定提交还是回滚事务
* 许多XA的实现都有一个叫做启发式决策（heuristic decistions）的紧急逃生舱口：允许参与者单方面决定放弃或提交一个存疑事务，而无需协调者做出最终决定。
* 这里启发式是可能破坏原子性（probably breaking atomicity）的委婉说法，因为它违背了两阶段提交的系统承诺。因此，启发式决策只是为了逃出灾难性的情况而准备的，而不是为了日常使用的。

### 分布式事务的限制 Limitations of distributed transactions
事务协调者本身就是一种数据库（存储了事务的结果），因此需要像其他重要数据库一样小心地打交道

## 容错共识
### 共识算法和全序广播
### 单领导者复制和共识
### 时代编号和法定人数
### 共识的局限性

## 成员与协调服务
### 将工作分配给节点
### 服务发现
### 成员服务

# 本章小结


